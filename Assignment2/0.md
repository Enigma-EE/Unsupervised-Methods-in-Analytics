### Understanding the Assignment Requirements

Your assignment focuses on three main tasks:

1. Conduct a literature review on anomaly detection methods with an emphasis on clustering techniques.
2. Design an anomaly detection algorithm based on k-means.
3. Apply this algorithm to a given dataset (`a2-housing.csv`) using SAS Enterprise Miner.

The deliverables include a paper, presentation slides, and a recorded presentation. The paper itself is divided into specific sections, including an introduction, literature review, algorithm description, and application of the algorithm to the dataset. The assignment accounts for 35% of the total course marks and is due on November 5, 2023, 11:59 pm Adelaide time.

### Paper Structure

1. **Introduction (10%)**
    - What is Anomaly Detection
    - Motivation and Examples
    - Main categories of anomaly detection methods, including the basic idea of each type of methods and suitable scenarios to use (data types, dimensionality etc.). 


what are the Main categories of anomaly detection methods used in this paper? including the basic idea of each type of methods and suitable scenarios to use (data types, dimensionality etc.).

### Anomaly Detection Using Unsupervised Methods vs. Traditional Rule-Based Systems

#### Unsupervised Methods: 

1. **Scalability**: Unsupervised methods like K-means or DBSCAN scale well with the size of the data. 
   
2. **Adaptability**: These methods can adapt to unseen or evolving data patterns, provided that the algorithm is periodically retrained.
  
3. **Complex Relationships**: Capable of capturing complex relationships between variables, including non-linearities.

4. **Minimal Domain Knowledge**: No explicit rules are needed, which is beneficial when domain expertise is lacking.

5. **False Positives**: May generate false positives if the model isn't well-tuned or if the underlying assumptions (like cluster shapes in K-means) don't hold.

6. **Interpretability**: Usually less interpretable than rule-based systems, unless additional steps are taken for feature selection or model explanation.

#### Traditional Rule-Based Systems:

1. **Simplicity**: Rule-based systems are straightforward to implement and understand.

2. **Domain Knowledge**: These systems usually rely heavily on domain expertise to formulate rules.

3. **Lack of Adaptability**: Static rules can quickly become outdated and fail to adapt to new kinds of anomalies unless manually updated.

4. **Scalability**: As the number of rules grows, the system may become computationally expensive to maintain.

5. **Comprehensiveness**: The system is only as good as the rules it contains; it may miss anomalies that have not been anticipated.

6. **Interpretability**: Rule-based systems are highly interpretable because the logic behind the decision-making process is transparent.

#### Comparative Summary:

1. **Flexibility**: Unsupervised methods offer more flexibility in capturing unknown or evolving anomaly patterns, while rule-based systems are rigid but easy to interpret.

2. **Domain Knowledge**: Rule-based systems require extensive domain knowledge; unsupervised methods do not.

3. **Maintenance**: Rule-based systems require continuous manual updates, whereas unsupervised methods require periodic retraining, which can be automated.

4. **Computational Load**: Rule-based systems may have a lower computational load for a small set of rules but can become inefficient as the number of rules grows. In contrast, unsupervised methods are inherently more scalable but may require more computational resources upfront.

In summary, the choice between unsupervised and rule-based methods for anomaly detection largely depends on the specific requirements of the task, including scalability, adaptability, and the availability of domain knowledge.

---


Guo, Pu., Lijuan, Wang., Jun, Shen., Fang, Dong. (2021). A Hybrid Unsupervised Clustering-Based Anomaly Detection Method. Tsinghua Science & Technology, 26(2):146-153. doi: 10.26599/TST.2019.9010051




**Main categories of anomaly detection methods used in this paper:**

- **Unsupervised Machine Learning Techniques**: Unsupervised machine learning techniques are particularly appealing for intrusion detection systems as they can detect known and unknown types of attacks as well as zero-day attacks  . The proposed approach in this paper combines Sub-Space Clustering (SSC) and One Class Support Vector Machine (OCSVM) to detect attacks without any prior knowledge  .

- **Sub-Space Clustering (SSC)**: SSC is a technique used in the proposed approach to identify clusters or subspaces in the data. It helps in identifying patterns and anomalies in the network traffic instances .

- **One Class Support Vector Machine (OCSVM)**: OCSVM is another technique used in the proposed approach for anomaly detection. It is a type of support vector machine that is trained on only one class of data (normal instances) and can identify deviations from this normal behavior as anomalies .

- **Suitable Scenarios**: The proposed approach using SSC and OCSVM is suitable for intrusion detection in scenarios where there is a need to detect both known and unknown types of attacks, including zero-day attacks. It can be applied to network traffic data, such as the NSL-KDD dataset used in this paper, to detect anomalies and potential cyber intrusions  .


- Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the normal behavior or expected patterns in a dataset. It is used to detect unusual or suspicious activities that may indicate cyber intrusions or fraudulent behavior.
- Motivation for anomaly detection: The increasing number and complexity of new attacks in recent years have made it necessary to develop effective and intelligent solutions for intrusion detection. Unsupervised machine learning techniques, such as anomaly detection, are particularly appealing as they can detect both known and unknown types of attacks, including zero-day attacks.
- Examples of anomaly detection: Anomaly detection can be applied in various domains, such as network security, fraud detection, system monitoring, and industrial quality control. For example, in network security, anomaly detection can help identify unusual network traffic patterns that may indicate a cyber attack. In fraud detection, it can help identify abnormal financial transactions that may indicate fraudulent activity.

**Categories of Anomaly Detection Methods:**
- Statistical-based methods: These methods use statistical techniques to model the normal behavior of the data and identify instances that deviate significantly from the expected patterns. Examples include Gaussian distribution-based methods and clustering-based methods.
- Machine learning-based methods: These methods use machine learning algorithms to learn the normal behavior of the data and detect anomalies based on deviations from the learned patterns. Examples include one-class support vector machines (OCSVM), neural networks, and decision trees.

- The paper presents a hybrid unsupervised anomaly detection method that combines Sub-Space Clustering (SSC) and One Class Support Vector Machine (OCSVM) to detect cyber attacks without prior knowledge. 
- The proposed method is evaluated using the NSL-KDD dataset, and the experimental results demonstrate its superior performance compared to existing techniques. 


- Machine learning-based intrusion detection methods use algorithms to learn patterns and behaviors from data in order to detect cyber attacks.  
- These methods typically fall into two categories: supervised and unsupervised learning. Supervised learning methods require labeled data, where the algorithm is trained on known attack and normal behavior examples. Unsupervised learning methods, on the other hand, do not require labeled data and can detect both known and unknown types of attacks.  
- In unsupervised learning, anomaly detection techniques are commonly used. These techniques aim to identify instances that deviate significantly from the normal behavior or expected patterns in a dataset. Statistical-based methods and machine learning-based methods, such as one-class support vector machines (OCSVM), are commonly used for anomaly detection in intrusion detection systems.  
- The machine learning algorithms analyze various features and characteristics of the data, such as network traffic patterns or system logs, to identify anomalies that may indicate a cyber attack. The algorithms learn from the data and adapt to new attack patterns, making them effective in detecting both known and unknown attacks.  


Advantages of using unsupervised machine learning techniques for intrusion detection:

- **Detection of known and unknown attacks**: Unsupervised machine learning techniques can detect both known and unknown types of attacks, including zero-day attacks, without relying on prior knowledge  .
- **Flexibility and adaptability**: Unsupervised techniques can adapt to new attack patterns and behaviors, making them effective in dynamic and evolving threat landscapes .
- **Reduced reliance on labeled data**: Unlike supervised learning methods, unsupervised techniques do not require labeled data for training, which can be challenging and time-consuming to obtain in the context of intrusion detection .
- **Detection of subtle anomalies**: Unsupervised techniques can identify subtle anomalies that may go unnoticed by traditional rule-based or signature-based approaches, improving the detection of sophisticated attacks .
- **Scalability**: Unsupervised techniques can handle large volumes of data and scale well to high-speed networks, making them suitable for real-time intrusion detection .
- **Ability to detect emerging threats**: Unsupervised techniques can detect emerging attack patterns and behaviors that may not be captured by predefined rules or signatures, enhancing the ability to detect novel and evolving threats .


The proposed method combines Sub-Space Clustering (SSC) and One Class Support Vector Machine (OCSVM) to detect attacks without any prior knowledge.

- **Sub-Space Clustering (SSC)**: SSC is a technique that partitions data into subspaces based on their similarity. It aims to identify clusters within the data by considering the relationships between different features or dimensions. SSC helps in capturing the underlying structure of the data and identifying patterns that may indicate attacks.

- **One Class Support Vector Machine (OCSVM)**: OCSVM is a machine learning algorithm that is used for anomaly detection. It learns the boundaries of normal behavior and classifies instances that fall outside these boundaries as anomalies. OCSVM is particularly useful in detecting unknown or novel attacks.

In the proposed method, SSC is used to identify subspaces or clusters within the data, and OCSVM is applied to each subspace to detect anomalies. By combining these two techniques, the method can effectively detect attacks without prior knowledge  .

**The proposed approach was evaluated using the NSL-KDD dataset**. The NSL-KDD dataset is a well-known dataset commonly used for evaluating intrusion detection systems.The experimental results demonstrated that the proposed method outperformed some of the existing techniques, indicating its effectiveness in detecting anomalies in network traffic.

The method combines Sub-Space Clustering (SSC) and One Class Support Vector Machine (OCSVM) to detect attacks without any prior knowledge . By using the well-known NSL-KDD dataset for evaluation, the proposed approach was able to assess its performance in detecting different types of attacks . The results showed that the method outperformed some of the existing techniques, indicating its effectiveness in detecting anomalies in network traffic . This suggests that the combination of SSC and OCSVM in the proposed method provides a more effective and intelligent solution for cyber intrusion detection, particularly in detecting known and unknown types of attacks as well as zero-day attacks.



---


Y. Qiu, T. Misu and C. Busso, "Unsupervised Scalable Multimodal Driving Anomaly Detection," in IEEE Transactions on Intelligent Vehicles, vol. 8, no. 4, pp. 3154-3165, April 2023, doi: 10.1109/TIV.2022.3160861.



**Main categories of anomaly detection methods used in this paper:**

- Supervised approaches: These methods effectively identify aspects related to driving anomalies but are unfeasible to tabulate and address all potential driving anomalies. They require labeled data for training and are suitable for scenarios where specific types of driving anomalies are known and can be labeled.

- Unsupervised approaches: These methods automatically identify unexpected driving scenarios without the need for labeled data. The proposed approach in the paper falls under this category. It formulates the detection of driving anomalies as a binary discrimination task between expected and unexpected driving behaviors. It uses unsupervised contrastive learning with conditional GANs and the triplet loss function to extract latent features from multiple modalities and discriminate normal and abnormal driving segments. Unsupervised approaches are suitable for scenarios where a wide range of driving anomalies can occur and labeled data is not available.

- Scalable formulation: The proposed approach also focuses on scalability, allowing for the easy addition of new modalities. This makes it suitable for scenarios where new data sources or modalities can be incorporated to enhance the detection of driving anomalies.

Overall, the paper primarily focuses on unsupervised approaches and scalability in driving anomaly detection.

**Methods used in this paper:**

- Unsupervised contrastive method using conditional generative adversarial networks (GANs) implemented with the attention model and the triplet loss function  
- Five different modalities considered: vehicle's CAN-Bus signals, driver's physiological signals, distance to nearby pedestrians, distance to nearby vehicles, and distance to nearby bicycles 
- Conditional GAN used to extract latent features from each modality 
- Attention model used to combine the latent representations from the modalities 
- Entire framework trained with the triplet loss function to generate effective representations for discriminating normal and abnormal driving segments 
- Experimental evaluations conducted on the driving anomaly dataset, achieving improved performance over alternative approaches.


**Anomaly Detection:**

- Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset .
- Motivation: Anomaly detection is motivated by the need to identify unusual or abnormal events, objects, or actions that can pose risks or indicate potential problems in various domains, including driving, cybersecurity, finance, and healthcare .
- Examples: In the context of driving, anomaly detection can help identify unexpected driving behaviors, such as sudden lane changes, aggressive acceleration or braking, or interactions with pedestrians or other vehicles that may increase the risk of accidents .
- Categories of Anomaly Detection Methods: Anomaly detection methods can be categorized into supervised and unsupervised approaches. Supervised methods require labeled data with both normal and anomalous instances for training, while unsupervised methods aim to automatically identify anomalies without prior knowledge of specific anomalies .

Overall, anomaly detection is a process of identifying deviations from the norm, motivated by the need to detect abnormal events or behaviors. In the context of driving, anomaly detection can help identify unexpected driving scenarios that may pose risks. Anomaly detection methods can be categorized into supervised and unsupervised approaches, with unsupervised methods being particularly relevant for automatically identifying anomalies without prior knowledge of specific anomalies.

**How does the proposed unsupervised contrastive method work?**

- The proposed unsupervised contrastive method uses conditional generative adversarial networks (GANs) implemented with the attention model and the triplet loss function to detect driving anomalies .
- The method involves training a conditional GAN to extract latent features from different modalities, including the vehicle's CAN-Bus signals, driver's physiological signals, and distances to nearby pedestrians, vehicles, and bicycles .
- An attention model is then used to combine the latent representations from these modalities, allowing for the fusion of information from multiple sources .
- The entire framework is trained with the triplet loss function, which generates effective representations to discriminate normal and abnormal driving segments .
- By formulating the detection of driving anomalies as a binary discrimination task between expected and unexpected driving behaviors, the proposed method can automatically identify unexpected driving scenarios without the need for labeled data .

Overall, the proposed unsupervised contrastive method utilizes conditional GANs, attention models, and the triplet loss function to extract and combine latent features from multiple modalities for the detection of driving anomalies.

Overall, the attention model in the proposed method combines latent representations from different modalities by learning how to weight the representations and captures contextual information to detect driving anomalies, even in scenarios where the driver may be unaware of potential anomalies.

**How is the attention model used to combine latent representations?**

- The attention model is used to fuse the latent representations from different modalities in the proposed unsupervised contrastive method for driving anomaly detection .
- It learns from the data how to weight the representations from each modality, allowing for the combination of information from multiple sources .
- The attention module is stacked multiple times, and it uses a feed-forward network with fully connected layers to facilitate residual connections .
- The attention model helps in capturing contextual information derived from vision-based object detection systems applied to the road, enabling the detection of anomalous scenarios even when the driver is unaware of them .
- By incorporating modalities associated with the environment, such as distances to nearby pedestrians, vehicles, and bicycles, the attention model enhances the ability to react to driving anomalies .

Overall, the attention model in the proposed method combines latent representations from different modalities by learning how to weight the representations and captures contextual information to detect driving anomalies, even in scenarios where the driver may be unaware of potential anomalies.

**What is the purpose of using a triplet loss function in training?**

- The purpose of using a triplet loss function in training is to generate effective representations that can discriminate normal and abnormal driving segments in the proposed unsupervised contrastive method for driving anomaly detection  .
- The triplet loss function is a contrastive-based metric that indicates the anomaly level of the target recording .
- It helps in training the model to learn a metric space where the representations of normal driving segments are closer to each other, while the representations of abnormal driving segments are farther apart .
- By optimizing the triplet loss function, the model can learn to create distinct clusters for normal and abnormal driving behaviors, enabling accurate detection of driving anomalies .
- The use of the triplet loss function allows the model to learn discriminative features that can effectively differentiate between expected and unexpected driving behaviors, without the need for labeled data .

In summary, the triplet loss function is used in training to generate effective representations and learn a contrastive-based metric that can discriminate between normal and abnormal driving segments, enabling the detection of driving anomalies in an unsupervised manner.


**How does this approach improve performance over alternative methods?**

- The proposed approach for driving anomaly detection using unsupervised contrastive learning with conditional generative adversarial networks (GANs) and the triplet loss function achieves improved performance over alternative methods  .
- By formulating the detection of driving anomalies as a binary discrimination task between expected and unexpected driving behaviors, the approach can automatically identify unexpected driving scenarios .
- The use of conditional GANs allows for the extraction of latent features from multiple modalities, including the vehicle's CAN-Bus signals, driver's physiological signals, and distances to nearby pedestrians, vehicles, and bicycles .
- The attention model is employed to combine the latent representations from these modalities, capturing contextual information and enhancing the ability to react to driving anomalies .
- The triplet loss function is utilized to generate effective representations that can discriminate normal and abnormal driving segments, enabling accurate detection of driving anomalies .
- Experimental evaluations on the driving anomaly dataset demonstrate the improved performance of the proposed approach compared to alternative methods .

In summary, the proposed approach improves performance over alternative methods by leveraging unsupervised contrastive learning, conditional GANs, the attention model, and the triplet loss function to automatically identify unexpected driving scenarios and accurately detect driving anomalies using multiple modalities.


---


M. Zhao, R. Furuhata, M. Agung, H. Takizawa and T. Soma, "Failure Prediction in Datacenters Using Unsupervised Multimodal Anomaly Detection," 2020 IEEE International Conference on Big Data (Big Data), Atlanta, GA, USA, 2020, pp. 3545-3549, doi: 10.1109/BigData50022.2020.9378419.




**Main categories of anomaly detection methods used in this paper:**

- Conventional threshold-based anomaly detection methods: These methods consider each sensor independently and use a predefined threshold to detect anomalies. However, determining an optimal threshold for each type of sensor can be challenging, especially in large-scale systems in datacenters  .

- Multimodal anomaly detection: This approach integrates sensing data from different types of sensors to detect failures that cannot be conventionally detected. It combines data from multiple sensors to capture different aspects of system behavior, such as temporal and spatial anomalies. The multimodal approach in this paper uses a correlation-based method to detect anomalies in a Network-Attached Storage (NAS) system with multiple hard disk drives (HDDs) and three sensors: a thermal camera, a microphone, and system performance logs  .

- Suitable scenarios: Multimodal anomaly detection is particularly useful in large-scale systems in datacenters, where there is a need to analyze data from multiple sensors to gain a comprehensive understanding of system behavior. It can be applied to scenarios where different types of sensors provide complementary information, such as detecting temporal anomalies using auditory and system performance data, and spatial anomalies using thermal data  .

Note: The provided sources do not contain a comprehensive literature survey beyond the information mentioned above.

**Literature survey of this paper:**

- The paper proposes a correlation-based multimodal anomaly detection approach for predicting hard drive failures in datacenters using sensing data from different types of sensors. It addresses the limitations of conventional threshold-based anomaly detection methods that consider each sensor independently .
- The approach is applied to a Network-Attached Storage (NAS) system with multiple hard disk drives (HDDs) and three sensors: a thermal camera, a microphone, and system performance logs .
- The unimodal results show that the auditory and system performance model can detect temporal anomalies, and the thermal model can detect spatial anomalies .
- The multimodal results demonstrate that the multimodal approach, even with a simple filter and detection algorithms, was able to detect failure signs before the real failure and earlier than the auditory unimodal approach .

**Anomaly Detection:**
- Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the normal behavior or expected patterns in a dataset. It is commonly used for predicting failures in datacenters by detecting abnormal behavior in sensing data.
- Anomaly detection is motivated by the need to avoid wasting resources and waiting time for recovery in datacenters. By predicting failures, proactive measures can be taken to prevent downtime and minimize the impact on operations.
- Examples of anomaly detection include predicting hard drive failures in datacenters, detecting fraudulent transactions in financial systems, identifying network intrusions, and monitoring equipment performance in industrial settings.

**Categories of Anomaly Detection Methods:**
- Threshold-based methods: These methods set a threshold value and classify instances as normal or anomalous based on whether they fall below or above the threshold. However, determining an optimal threshold can be challenging, especially for large-scale systems in datacenters.
- Multimodal methods: These methods integrate sensing data from different types of sensors to detect failures that cannot be conventionally detected. By considering correlations between different sensor modalities, multimodal anomaly detection can provide more accurate and early detection of failure signs.

**Limitations of conventional threshold-based anomaly detection methods:**
- Conventional threshold-based anomaly detection methods consider each sensor independently, which can be problematic for large-scale systems in datacenters .
- Determining an optimal threshold for each type of sensor is not trivial, especially in complex and dynamic environments .
- Threshold-based methods may not be able to effectively detect anomalies that do not fall within the predefined threshold, leading to false negatives or missed detections .


**Why is multimodal anomaly detection important for large-scale systems in datacenters?**

- Multimodal anomaly detection is important for large-scale systems in datacenters because it allows for the integration of sensing data from different types of sensors, providing a more comprehensive and accurate understanding of the system's behavior and potential failures .
- Conventional threshold-based anomaly detection methods that consider each sensor independently may not be sufficient for large-scale systems, as determining an optimal threshold for each type of sensor can be challenging .
- By combining data from multiple sensors, multimodal anomaly detection can capture different aspects of system behavior, such as temporal and spatial anomalies, leading to improved detection capabilities .
- Multimodal anomaly detection can help detect failure signs before the actual failure occurs and potentially provide earlier warnings compared to unimodal approaches, as demonstrated in the study .

**Comparison of multimodal and unimodal approaches in detecting failure signs:**

- The unimodal results of the study showed that the auditory and system performance model could detect temporal anomalies, while the thermal model could detect spatial anomalies .
- However, the multimodal results demonstrated that the multimodal approach, even with simple filter and detection algorithms, was able to detect failure signs before the actual failure and earlier than the auditory unimodal approach .


---

S. Chen, X. Li and L. Zhao, "Hyperspectral Anomaly Detection with Data Sphering and Unsupervised Target Detection," IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium, Kuala Lumpur, Malaysia, 2022, pp. 1975-1978, doi: 10.1109/IGARSS46834.2022.9884083.




**Main categories of anomaly detection methods used in this paper:**

- **Low rank and sparse representation (LRaSR)-based approaches**: These methods decompose hyperspectral data into low-rank components for background, sparse components for anomalies, and residual components for noise. They are suitable for hyperspectral data with low-rank background and sparse anomalies  .

- **Autoencoder (AE)-based methods**: These methods use neural networks to learn the underlying structure of the data and reconstruct it. Anomalies are detected by measuring the reconstruction error. AE-based methods are suitable for high-dimensional data and can capture complex patterns .

- **Data sphering and unsupervised target detection with sparse cardinality (DS-UTSSC)**: This novel method proposed in the paper combines data sphering, unsupervised target detection, and sparse cardinality. It removes the background using data sphering, generates a potential anomaly component through unsupervised target detection, and incorporates sparse cardinality to reduce noise impact. DS-UTSSC is competitive against LRaSR-based models and AE-based methods in hyperspectral anomaly detection  .

These methods are suitable for hyperspectral data with different characteristics, such as low-rank background, sparse anomalies, and high dimensionality. They offer different approaches to detect anomalies and can be applied in various scenarios depending on the specific data types and dimensionality.



**Literature survey of this paper:**

- The paper proposes a novel hyperspectral anomaly detection (AD) method called DS-UTSSC, which combines data sphering (DS) and unsupervised target detection with sparse cardinality (DS-UTSSC) .
- The method first uses data sphering to remove the background (BKG) from the original data .
- Then, it generates a potential anomaly component using unsupervised target detection and subspace projection for the sphered data .
- To further reduce the impact of noise on anomaly detection, sparse cardinality is incorporated .
- Finally, RX-AD is implemented on the obtained component to detect anomalies .
- The experimental results show that DS-UTSSC is competitive against other existing models, such as LRaSR-based models and AE-based methods .

**Methods used in this paper:**

- The paper proposes a novel hyperspectral anomaly detection (AD) method called DS-UTSSC, which combines data sphering (DS) and unsupervised target detection with sparse cardinality (DS-UTSSC) .
- The method first uses data sphering to remove the background (BKG) from the original data .
- Then, it generates a potential anomaly component using unsupervised target detection and subspace projection for the sphered data .
- To further reduce the impact of noise on anomaly detection, sparse cardinality is incorporated .
- Finally, RX-AD is implemented on the obtained component to detect anomalies .

**Practical implications of this paper:**

- The proposed DS-UTSSC method for hyperspectral anomaly detection has practical implications in various fields such as remote sensing, surveillance, and target detection.
- By effectively decomposing hyperspectral data into background, anomalies, and noise components, the DS-UTSSC method can accurately detect anomalies in hyperspectral images .
- The use of data sphering helps in removing the background from the original data, improving the accuracy of anomaly detection .
- Incorporating sparse cardinality further reduces the impact of noise on anomaly detection, enhancing the reliability of the detection results .
- The experimental results demonstrate that DS-UTSSC is competitive against other existing models, such as LRaSR-based models and AE-based methods, indicating its potential practical utility in real-world applications .

**Advantages of using low rank and sparse representation in hyperspectral anomaly detection:**

- Low rank and sparse representation (LRaSR) approaches effectively decompose hyperspectral data into a low-rank component for background, a sparse component for anomalies, and a residual component for noise .
- The main advantage of using LRaSR in hyperspectral anomaly detection is its ability to separate anomalies from the background and noise, leading to improved detection accuracy .
- The low-rank component captures the underlying structure of the background, while the sparse component represents the anomalies, which are typically present in a limited number of spectral bands .
- By utilizing the sparsity of anomalies, LRaSR-based methods can effectively distinguish them from the background and noise, enhancing the detection performance .
- LRaSR approaches have been widely used in hyperspectral anomaly detection and have shown competitive performance compared to other methods, making them a valuable tool in various applications such as remote sensing and surveillance .

**The proposed DS-UTSSC method improves hyperspectral anomaly detection in the following ways:**

- **Data sphering (DS):** The method utilizes data sphering to remove the background from the original hyperspectral data, enhancing the detection of anomalies .
- **Unsupervised target detection:** The method employs unsupervised target detection to generate a potential anomaly component by analyzing the sphered data and projecting it onto a subspace .
- **Sparse cardinality (SC):** To reduce the impact of noise on anomaly detection, the method incorporates sparse cardinality, which further refines the potential anomaly component .
- **RX-AD implementation:** Finally, the method applies RX-AD (Reed-Xiaoli anomaly detection) on the refined potential anomaly component to detect anomalies .
- **Competitive performance:** Experimental results validate that the DS-UTSSC method is highly competitive against other approaches, such as LRaSR-based models and AE-based methods, in terms of hyperspectral anomaly detection .

Overall, the DS-UTSSC method improves hyperspectral anomaly detection by effectively removing the background, identifying potential anomalies, reducing noise impact, and implementing a robust anomaly detection algorithm.

**Unsupervised target detection plays a crucial role in generating the potential anomaly component in the proposed DS-UTSSC method for hyperspectral anomaly detection**. Here is how it contributes:

- Unsupervised target detection is used to identify potential anomalies in the hyperspectral data without the need for labeled training samples or prior knowledge of the anomalies.
- The unsupervised target detection algorithm analyzes the sphered data, obtained after removing the background using data sphering, and projects it onto a subspace to generate the potential anomaly component.
- This process helps in separating the anomalies from the background and noise, allowing for a more accurate detection of anomalies in the hyperspectral data.

Overall, unsupervised target detection plays a crucial role in identifying and generating the potential anomaly component, which is further processed to detect anomalies in the DS-UTSSC method for hyperspectral anomaly detection.

**Incorporating sparse cardinality reduces noise impact on anomaly detection in the following ways:**

- Sparse cardinality helps in selecting a subset of the most relevant features or components from the potential anomaly component, which reduces the influence of noisy or irrelevant information on anomaly detection .
- By incorporating sparse cardinality, the DS-UTSSC method focuses on identifying anomalies based on the sparse representation of the data, which can effectively suppress the impact of noise and enhance the detection of true anomalies .
- The use of sparse cardinality allows for a more precise and targeted identification of anomalies by emphasizing the most significant and informative components while disregarding the noisy or less relevant ones .

Overall, incorporating sparse cardinality in the DS-UTSSC method helps to mitigate the impact of noise on anomaly detection by selecting the most relevant features and components, leading to more accurate and reliable detection of anomalies in hyperspectral data.

---

S. Shriram and E. Sivasankar, "Anomaly Detection on Shuttle data using Unsupervised Learning Techniques," 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE), Dubai, United Arab Emirates, 2019, pp. 221-225, doi: 10.1109/ICCIKE47802.2019.9004325.


**Main categories of anomaly detection methods used in this paper:**

- **Unsupervised Learning Techniques**: The paper compares various unsupervised anomaly detection techniques, including One Class Support Vector Machine (OneClassSVM), Local Outlier Factor (LOF), Isolation Forest (IF), and Elliptic Envelope (EE) . These techniques are suitable for scenarios where labeled data is scarce or unavailable, and they do not rely on pre-defined labels or assumptions about the data distribution. They can be effective in detecting unknown or novel anomalies and are adaptable to changing data patterns. Unsupervised learning techniques are well-suited for practical applications where data used will have no labels and where flexibility and scalability are important factors .

- **Supervised Learning Techniques**: The paper also compares the performance of unsupervised learning techniques with supervised learning techniques such as Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN) . Supervised learning techniques require labeled data for training and may not perform as well in scenarios with limited labeled data or unknown anomalies. They are suitable when labeled data is available and can be used for classification of anomalies based on the known patterns in the data .

Overall, the paper focuses on comparing the performance of unsupervised learning techniques with supervised learning techniques for anomaly detection, highlighting the effectiveness of unsupervised techniques in scenarios where labeled data is limited or unavailable .




**Literature survey of this paper:**

- The paper focuses on comparing various unsupervised anomaly detection techniques using performance metrics like precision, recall, F-score, and area under the curve .
- The unsupervised learning techniques used in this work are One Class Support Vector Machine (OneClassSVM), Local Outlier Factor (LOF), Isolation Forest (IF), and Elliptic Envelope (EE) .
- The performance of these unsupervised learning techniques was compared with supervised learning techniques such as SVM and k-NN .
- Results show that unsupervised learning techniques are on par or better for anomaly detection compared to supervised learning techniques for the shuttle and satellite datasets .

**Performance metrics used to compare unsupervised anomaly detection techniques:**

- Precision, recall, F-score, and area under the curve were used as performance metrics to compare the unsupervised anomaly detection techniques .

**Unsupervised learning technique for anomaly detection:**

- The paper compares various unsupervised anomaly detection techniques, including One Class Support Vector Machine (OneClassSVM), Local Outlier Factor (LOF), Isolation Forest (IF), and Elliptic Envelope (EE) .
- The results of the comparison show that unsupervised learning techniques are on par or better for anomaly detection compared to supervised learning techniques for the shuttle and satellite datasets .
- However, the paper does not explicitly mention which unsupervised learning technique performed the best for anomaly detection.
- Therefore, based on the information provided in the sources, it is not possible to determine which unsupervised learning technique performed the best for anomaly detection.

**Comparison of supervised and unsupervised learning techniques for anomaly detection:**

- In the context of anomaly detection, supervised learning techniques like Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN) were compared to unsupervised learning techniques.


- The paper states that unsupervised learning techniques are on par or better for anomaly detection compared to supervised learning techniques for the shuttle and satellite datasets.


- This suggests that the unsupervised learning techniques, such as One Class Support Vector Machine (OneClassSVM), Local Outlier Factor (LOF), Isolation Forest (IF), and Elliptic Envelope (EE), performed well in identifying anomalies without the need for labeled data.


- Overall, the results indicate that unsupervised learning techniques are well-suited for anomaly detection in scenarios where labeled data is not available.

**Datasets used for experimentation in this research paper:**

- The research paper used the Shuttle and satellite datasets for experimentation in the context of anomaly detection .

- These datasets were utilized to compare the performance of various unsupervised learning techniques, such as One Class Support Vector Machine (OneClassSVM), Local Outlier Factor (LOF), Isolation Forest (IF), and Elliptic Envelope (EE), with supervised learning techniques like Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN) .

- The specific details about the characteristics and composition of the Shuttle and satellite datasets are not provided in the available sources.

- However, it can be inferred that these datasets were chosen as representative examples for evaluating the effectiveness of different anomaly detection techniques in real-world applications.
  
**Practical applications where unsupervised learning outperforms supervised learning for anomaly detection:**

- Unsupervised learning techniques can be advantageous in scenarios where labeled data is scarce or unavailable, making them well-suited for anomaly detection in such cases .

- Anomaly detection in real-world applications often involves identifying unknown or novel anomalies that may not have been encountered during the training phase. Unsupervised learning techniques can be more effective in detecting these unknown anomalies as they do not rely on pre-defined labels or assumptions about the data distribution .

- Unsupervised learning techniques can also be more flexible and adaptable to changing data patterns, as they do not require prior knowledge or supervision. This makes them suitable for dynamic environments where anomalies may evolve over time .

- Additionally, unsupervised learning techniques can be more efficient and scalable for large-scale datasets, as they do not require the manual labeling of data instances. This can save time and resources in the data annotation process .

- Overall, unsupervised learning techniques offer a valuable approach for anomaly detection in practical applications where labeled data is limited, unknown anomalies need to be detected, and adaptability to changing data patterns is crucial .


**Anomaly Detection:**

- Anomaly detection refers to the process of identifying observations or data points that deviate significantly from the normal behavior or patterns in a given dataset .

**Motivation and Examples:**

- The motivation behind anomaly detection is to detect and identify unusual or suspicious activities, events, or data points that may indicate potential fraud, errors, or anomalies in various domains such as cybersecurity, finance, healthcare, and industrial systems .

- Examples of anomaly detection include detecting fraudulent transactions in financial transactions, identifying network intrusions or cyber attacks, detecting anomalies in medical data for disease diagnosis, and identifying anomalies in industrial processes for predictive maintenance .

**Categories of Anomaly Detection Methods:**

- Anomaly detection methods can be categorized into supervised and unsupervised learning techniques. In supervised learning, labeled data is used to train a model to classify anomalies. Unsupervised learning techniques, on the other hand, do not rely on labeled data and aim to identify anomalies based on the underlying patterns or deviations from normal behavior in the data .

- Common unsupervised anomaly detection methods include One Class Support Vector Machine (OneClassSVM), Local Outlier Factor (LOF), Isolation Forest (IF), and Elliptic Envelope (EE) .

- Supervised learning techniques such as Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN) can also be used for anomaly detection, but they require labeled data for training and may not perform as well in scenarios with limited labeled data or unknown anomalies .

----


    - What is Anomaly Detection
    - Motivation and Examples
    - Categories of Anomaly Detection Methods


M. P. Handayani, G. Antariksa and J. Lee, "Anomaly Detection in Vessel Sensors Data with Unsupervised Learning Technique," 2021 International Conference on Electronics, Information, and Communication (ICEIC), Jeju, Korea (South), 2021, pp. 1-6, doi: 10.1109/ICEIC51217.2021.9369822.



**Main categories of anomaly detection methods used in this paper:**

- **Isolation Forest**: This unsupervised learning technique randomly selects features and creates isolation trees to isolate anomalies in the data. It is suitable for high-dimensional datasets and can efficiently handle large amounts of data. It is effective in detecting outliers or anomalies in sensor data  .

- **t-SNE**: This technique is used to reduce the dimensionality of the data. t-SNE stands for t-Distributed Stochastic Neighbor Embedding and it is commonly used for visualizing high-dimensional data. It helps in representing complex relationships between data points in a lower-dimensional space, making it easier to detect anomalies .

- Suitable scenarios to use these methods: 
  - Isolation Forest is suitable for scenarios where there is a need to detect anomalies in high-dimensional datasets, such as sensor data from vessels. It can handle large amounts of data efficiently and is effective in identifying outliers or anomalies in the data  .
  - t-SNE is useful for reducing the dimensionality of the data, particularly in cases where there are complex relationships between data points. It can help in visualizing the data and identifying anomalies in a lower-dimensional space .





**Literature survey of this paper:**

- The paper focuses on anomaly detection in vessel sensor data using unsupervised learning techniques .
- It highlights the importance of detecting anomalies in sensor data to prevent engine failures and reduce maintenance costs .
- The Isolation Forest algorithm is used to detect anomalies in the sensor data .
- To reduce the dimensionality of the data, the t-SNE technique is adopted .


**Anomaly Detection:**

- Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset .
- Motivation for anomaly detection lies in its ability to identify unusual or suspicious events, which can be indicative of potential problems, threats, or opportunities for improvement .
- Examples of anomaly detection include identifying fraudulent transactions in financial systems, detecting network intrusions, monitoring equipment for faults or failures, and identifying anomalies in sensor data from vessels .
- Categories of anomaly detection methods include statistical-based methods, machine learning-based methods, and hybrid approaches that combine both techniques .

**How does the sensor system in a ship detect engine anomalies?**

- In a ship or vessel, a sensor system is used to indicate the engine status and detect any anomalies that may cause engine failures .
- The sensor system collects data from various sensors installed in the ship, such as temperature, pressure, vibration, and fuel consumption sensors.
- The collected sensor data is analyzed using unsupervised learning techniques, specifically the Isolation Forest algorithm, to detect anomalies in the data .
- Anomalies in the sensor data can indicate potential issues or malfunctions in the engine, such as abnormal temperature or pressure readings, unusual vibrations, or unexpected changes in fuel consumption.
- By detecting these anomalies, the sensor system can provide early warnings and recommendations for maintenance, helping to prevent engine failures and reduce maintenance costs .


**Purpose of using Isolation Forest in analyzing sensor data:**

- The Isolation Forest algorithm is used to detect anomalies in sensor data from vessels .
- It is employed to identify instances or patterns in the data that deviate significantly from the expected behavior or norm .
- The Isolation Forest algorithm is particularly suitable for high-dimensional datasets and can efficiently handle large amounts of data .
- It works by randomly selecting features and creating isolation trees to isolate anomalies, making it effective in detecting outliers or anomalies in the sensor data .
- By using the Isolation Forest algorithm, the system can identify potential anomalies in the sensor data, which can be indicative of engine malfunctions or issues .
- Detecting these anomalies allows for early warnings and recommendations for maintenance, contributing to the prevention of engine failures and reduction of maintenance costs .

**How can detecting anomaly contribute to reducing maintenance costs?**

- Detecting anomalies in sensor data allows for early identification of potential issues or malfunctions in the engine, enabling proactive maintenance interventions .
- By addressing these anomalies promptly, maintenance can be performed before the issues escalate into major failures, reducing the need for costly repairs or replacements .
- Timely maintenance based on anomaly detection helps prevent unexpected breakdowns, which can lead to expensive emergency repairs and downtime .
- Anomaly detection also enables the identification of patterns or trends in sensor data that may indicate underlying problems, allowing for targeted maintenance actions to address the root causes .
- By addressing the root causes of anomalies, the overall reliability and performance of the engine can be improved, leading to longer equipment lifespan and reduced maintenance costs in the long run .


---
    - What is Anomaly Detection
    - Motivation and Examples
    - Main categories of anomaly detection methods, including the basic idea of each type of methods and suitable scenarios to use (data types, dimensionality etc.). 

T. Zoppi, A. Ceccarelli and A. Bondavalli, "Into the Unknown: Unsupervised Machine Learning Algorithms for Anomaly-Based Intrusion Detection," 2020 50th Annual IEEE-IFIP International Conference on Dependable Systems and Networks-Supplemental Volume (DSN-S), Valencia, Spain, 2020, pp. 81-81, doi: 10.1109/DSN-S50200.2020.00044.




**Main categories of anomaly detection methods used in this paper:**

- **Statistical-based methods**: These methods use statistical techniques to model the normal behavior of the system and identify deviations from it. They assume that anomalies are rare events that can be detected by analyzing statistical properties of the data. Suitable for scenarios with well-defined statistical properties and low-dimensional data. 

- **Machine learning-based methods**: These methods utilize machine learning algorithms to learn patterns from the data and identify anomalies based on deviations from the learned patterns. They can handle high-dimensional data and are suitable for scenarios where the statistical properties of anomalies are not well-defined. 

- **Clustering-based methods**: These methods group similar instances together and identify instances that do not belong to any cluster as anomalies. They are suitable for scenarios where anomalies form distinct clusters or have different characteristics compared to normal instances. 

- **Neural network-based methods**: These methods use neural networks to learn the normal behavior of the system and detect anomalies based on deviations from the learned patterns. They can handle complex and non-linear relationships in the data and are suitable for scenarios with high-dimensional data. 

- **Graph-based methods**: These methods model the relationships between data instances as a graph and detect anomalies based on unusual patterns or connections in the graph. They are suitable for scenarios where anomalies can be identified through their relationships with other instances. 

- **Ensemble methods**: These methods combine multiple anomaly detection techniques to improve the overall detection accuracy. They are suitable for scenarios where different types of anomalies may require different detection approaches. 


Anomaly detection is a technique used to identify patterns in data that do not conform to expected behavior, and it has shown promise in detecting intrusions, zero-day attacks, and failures .
Unsupervised machine learning algorithms are particularly suited for anomaly detection as they can classify normal and anomalous behaviors without relying on labeled attack data .

**Anomaly Detection:**

- Anomaly detection is a technique used to identify patterns in data that do not conform to the expected behavior .
- The motivation behind anomaly detection is to detect unusual or suspicious activities that may indicate intrusions, zero-day attacks, or failures .
- Examples of anomaly detection include identifying unusual network traffic patterns, detecting fraudulent transactions, and spotting abnormal behavior in system logs .
- Anomaly detection methods can be categorized into supervised, unsupervised, and semi-supervised approaches .
- Supervised methods require labeled data with both normal and anomalous instances for training .
- Unsupervised methods, which are the focus of this tutorial, can classify normal and anomalous behaviors without relying on labeled attack data .
- Semi-supervised methods utilize a combination of labeled and unlabeled data for training .

**Main Goal of Anomaly Detection:**

- The main goal of anomaly detection is to identify patterns in data that deviate from the expected behavior, allowing for the detection of unusual or suspicious activities.


- Anomaly detection techniques are particularly useful in identifying intrusions, zero-day attacks, and failures that may go unnoticed by traditional rule-based systems.


- By leveraging machine learning algorithms, anomaly detection can classify normal and anomalous behaviors without relying on labeled attack data, making it suitable for unsupervised intrusion detection.


- Anomaly detection methods can be applied in various domains, such as network security, fraud detection, system monitoring, and predictive maintenance, to ensure the integrity and security of systems and data.

**Machine learning algorithms can be used for anomaly detection in the following ways:**

- **Unsupervised Learning:** Machine learning algorithms can be trained on a dataset containing normal behavior patterns to learn the expected behavior. Any deviation from this learned behavior can be flagged as an anomaly .

- **Feature Extraction:** Machine learning algorithms can extract relevant features from the data to identify anomalies. These features can capture patterns, trends, or statistical deviations that indicate abnormal behavior .

- **Clustering Techniques:** Machine learning algorithms can use clustering techniques to group similar instances together. Any instance that does not fit into any cluster or forms a separate cluster can be considered an anomaly .

- **Statistical Methods:** Machine learning algorithms can utilize statistical methods such as Gaussian distribution, probability density estimation, or outlier detection techniques to identify anomalies based on deviations from the expected statistical properties of the data .

- **Ensemble Methods:** Machine learning algorithms can combine multiple models or algorithms to improve the accuracy of anomaly detection. Ensemble methods can help in reducing false positives and increasing the detection rate .

- **Deep Learning:** Deep learning algorithms, such as autoencoders or recurrent neural networks, can be used for anomaly detection by learning the normal behavior patterns and identifying deviations from them .

- **Online Learning:** Machine learning algorithms can be adapted to continuously learn and update the expected behavior patterns in real-time, allowing for the detection of evolving anomalies .

- **Domain-Specific Approaches:** Machine learning algorithms can be tailored to specific domains, such as network security or fraud detection, by incorporating domain-specific knowledge and features to improve anomaly detection accuracy .

**Types of security threats that anomaly detection can help identify:**

- Intrusions: Anomaly detection can detect unauthorized access attempts or suspicious activities that deviate from normal behavior patterns, helping to identify potential intrusions .

- Zero-day attacks: Anomaly detection can identify previously unknown or unseen attack patterns, allowing for the detection of zero-day attacks that exploit vulnerabilities before they are known .

- Malware and viruses: Anomaly detection can detect unusual patterns or behaviors associated with malware or viruses, enabling the identification of infected systems or malicious activities .

- Insider threats: Anomaly detection can identify abnormal behaviors by authorized users, helping to detect insider threats such as data theft, unauthorized access, or misuse of privileges .

- Network anomalies: Anomaly detection can detect unusual network traffic patterns, such as high volumes of data transfers, unusual protocols, or abnormal communication patterns, indicating potential network attacks or anomalies .

- System failures: Anomaly detection can identify deviations from normal system behavior, such as resource exhaustion, abnormal system logs, or unexpected errors, helping to detect and prevent system failures or malfunctions .

- Fraud detection: Anomaly detection can identify unusual patterns or behaviors in financial transactions, customer behavior, or user activities, aiding in the detection of fraudulent activities or transactions .

**Unsupervised algorithms** are machine learning algorithms that do not require labeled data for training. They aim to identify patterns or anomalies in data without prior knowledge of the expected outcomes. In the context of intrusion detection, unsupervised algorithms are used to classify normal and anomalous behaviors without relying on input data with labeled attacks. 

**Differences between unsupervised and supervised algorithms in intrusion detection**:

- **Training data**: Supervised algorithms require labeled data, where each instance is labeled as normal or anomalous. Unsupervised algorithms, on the other hand, do not require labeled data for training.

- **Detection approach**: Supervised algorithms learn from labeled data to classify instances as normal or anomalous based on predefined patterns. Unsupervised algorithms, however, identify anomalies by learning the normal behavior patterns and flagging any deviations from them.

- **Flexibility**: Supervised algorithms are limited to detecting known attack patterns, while unsupervised algorithms can detect unknown or previously unseen anomalies, making them more suitable for detecting zero-day attacks or novel intrusion attempts.

- **Scalability**: Unsupervised algorithms can handle large-scale datasets without the need for manual labeling, making them more scalable compared to supervised algorithms.

- **Domain adaptation**: Unsupervised algorithms can adapt to different domains or environments without the need for retraining, making them more flexible in handling diverse intrusion detection scenarios.

**How does the use of unsupervised algorithms contribute to more accurate intrusion detection?**

- Unsupervised algorithms contribute to more accurate intrusion detection by allowing the identification of unknown or previously unseen anomalies, including zero-day attacks, which may not have labeled training data available .

- Unsupervised algorithms learn the normal behavior patterns from the data and flag any deviations from them, enabling the detection of abnormal activities that may indicate intrusion attempts or malicious behavior .

- These algorithms are flexible and adaptable to different domains or environments without the need for retraining, making them more effective in handling diverse intrusion detection scenarios .

- Unsupervised algorithms can handle large-scale datasets without the need for manual labeling, making them more scalable and efficient in detecting anomalies in real-time .

- By not relying on labeled attack data, unsupervised algorithms can detect novel attack patterns and adapt to evolving threats, enhancing the accuracy and effectiveness of intrusion detection systems .

---

    - What is Anomaly Detection
    - Motivation and Examples
    - Categories of Anomaly Detection Methods

A. B. Nassif, M. A. Talib, Q. Nasir and F. M. Dakalbab, "Machine Learning for Anomaly Detection: A Systematic Review," in IEEE Access, vol. 9, pp. 78658-78700, 2021, doi: 10.1109/ACCESS.2021.3083060.





**Main Categories of Anomaly Detection Methods:**

- **Unsupervised Methods**: These methods do not require labeled data and aim to identify anomalies based on deviations from normal patterns. They include statistical approaches like Gaussian Mixture Models (GMM) and clustering-based methods like k-means clustering. Unsupervised methods are suitable for scenarios where labeled data is scarce or unavailable  .

- **Supervised Methods**: These methods rely on labeled data to train a model that can classify anomalies. They involve techniques like Support Vector Machines (SVM) and Random Forests. Supervised methods are suitable when labeled data is available and anomalies can be clearly defined .

- **Semi-Supervised Methods**: These methods utilize a combination of labeled and unlabeled data for training. They aim to learn the normal patterns from the labeled data and identify deviations in the unlabeled data. Semi-supervised methods are suitable when a limited amount of labeled data is available .

- **Deep Learning Methods**: These methods leverage deep neural networks to automatically learn complex patterns and detect anomalies. They include techniques like Autoencoders and Variational Autoencoders. Deep learning methods are suitable for scenarios with high-dimensional data and complex patterns .




**Anomaly Detection:**
- Anomaly detection refers to the process of identifying and extracting anomalous components from data. It has been used for decades and plays an important role in various fields.
- Motivation: Anomaly detection is motivated by the need to identify unusual patterns or outliers in data that deviate from the expected behavior. It helps in detecting fraud, network intrusions, system failures, and other abnormal activities.
- Examples: Anomaly detection can be applied in various domains such as finance, cybersecurity, healthcare, manufacturing, and environmental monitoring. For instance, it can be used to detect fraudulent transactions, identify network attacks, detect anomalies in patient health data, identify faults in manufacturing processes, and detect anomalies in environmental sensor data.
- Categories of Anomaly Detection Methods: Anomaly detection methods can be categorized into supervised, unsupervised, and semi-supervised techniques. In the reviewed literature, unsupervised anomaly detection has been adopted more frequently by researchers compared to other classification-based methods.

**Main categories of anomaly detection methods:**

- **Supervised Anomaly Detection**: In this approach, anomalies are detected by training a model on labeled data, where anomalies are explicitly identified. It requires a labeled dataset with both normal and anomalous instances. Suitable for scenarios where labeled data is available and the types of anomalies are well-defined. 

- **Unsupervised Anomaly Detection**: This approach detects anomalies without the need for labeled data. It focuses on identifying patterns that deviate significantly from the normal behavior of the data. Suitable for scenarios where labeled data is scarce or unavailable, and the types of anomalies are unknown or constantly changing. 

- **Semi-Supervised Anomaly Detection**: This approach combines elements of both supervised and unsupervised methods. It uses a small amount of labeled data along with a larger amount of unlabeled data to detect anomalies. Suitable for scenarios where a limited amount of labeled data is available, but the majority of the data is unlabeled. 

- **Hybrid Anomaly Detection**: This approach combines multiple techniques, such as statistical methods, machine learning algorithms, and domain-specific knowledge, to detect anomalies. It leverages the strengths of different methods to improve detection accuracy. Suitable for scenarios where a combination of techniques can provide better anomaly detection performance. 

- **Time Series Anomaly Detection**: This approach focuses on detecting anomalies in time series data, where the temporal order of data points is important. It considers patterns and deviations over time to identify anomalies. Suitable for scenarios where anomalies occur over time, such as in financial data, sensor data, or network traffic. 

- **Network-based Anomaly Detection**: This approach analyzes the network structure and behavior to detect anomalies. It identifies unusual patterns or activities in network traffic, such as network intrusions or abnormal communication patterns. Suitable for scenarios where anomalies can be detected by analyzing network traffic or behavior. 


**Four Perspectives for Analyzing ML Models for Anomaly Detection:**
- Applications of anomaly detection in various domains were analyzed, resulting in the identification of 43 different applications of anomaly detection .
- ML techniques used for anomaly detection were analyzed, leading to the identification of 29 distinct ML models used in the identification of anomalies .
- Performance metrics for ML models in anomaly detection were analyzed, although specific details about these metrics are not provided in the available sources.
- The classification of anomaly detection methods was analyzed, with a focus on the observation that unsupervised anomaly detection has been adopted more frequently by researchers compared to other classification-based methods .

**Unsupervised anomaly detection** has been adopted more frequently by researchers compared to other classification-based methods.

---


    - What is Anomaly Detection
    - Motivation and Examples
    - Main categories of anomaly detection methods, including the basic idea of each type of methods and suitable scenarios to use (data types, dimensionality etc.). 

H. Deng and X. Li, "Anomaly Detection via Reverse Distillation from One-Class Embedding," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 9727-9736, doi: 10.1109/CVPR52688.2022.00951.



**Main Categories of Anomaly Detection Methods in the Paper:**

- **Statistical Methods**: These methods assume that normal data follows a specific statistical distribution, and anomalies are identified as data points that significantly deviate from this distribution. Suitable for low-dimensional data with known statistical properties.

- **Machine Learning Methods**: These methods use algorithms to learn patterns from normal data and identify anomalies as data points that do not conform to these learned patterns. Suitable for high-dimensional data with complex patterns.

- **Deep Learning Methods**: The paper proposes a novel teacher-student (T-S) model with a teacher encoder and a student decoder, leveraging deep neural networks to learn complex representations of normal data and identify anomalies based on deviations from these learned representations. Suitable for high-dimensional data with complex patterns and non-linear relationships.

**Classical Anomaly Detection Methods:**
- Classical anomaly detection methods include one-class support vector machine (OC-SVM) and support vector data description (SVDD) .
- DeepSVDD and PatchSVDD estimate data representations through deep networks to cope with high-dimensional data .

**Generative Models for Anomaly Detection:**
- Generative models like AutoEncoder (AE) and Generative Adversarial Nets (GAN) have been used for sample reconstruction in unsupervised anomaly detection .
- However, recent studies show that deep models can even successfully reconstruct anomalous regions, leading to the incorporation of memory mechanisms, image masking strategies, and pseudo-anomaly techniques in reconstruction-based methods .

**Comparison with Prior Arts:**
- The proposed method surpasses the state-of-the-art performance in unsupervised anomaly detection, as demonstrated through extensive experimentation on AD and one-class novelty detection benchmarks  .
- Prior arts in the experiments include LSA, OCGAN, HRN, DAAD, MKD, GT, GANomaly (GN), Uninformed Student (US), PSVDD, MetaFormer (MF), PaDiM (WResNet50), and CutPaste .

**Novelty of the Proposed Method:**
- The proposed method introduces a novel teacher-student (T-S) model consisting of a teacher encoder and a student decoder, with a reverse distillation paradigm .
- The student network takes the teacher model's one-class embedding as input and targets to restore the teacher's multiscale representations .
- A trainable one-class bottleneck embedding (OCBE) module is introduced in the T-S model, which effectively preserves essential information on normal patterns while abandoning anomaly perturbations  .

The paper focuses on unsupervised anomaly detection and proposes a novel teacher-student (T-S) model for this purpose. The T-S model consists of a teacher encoder and a student decoder, and the student network takes the teacher model's one-class embedding as input. The goal of the student network is to restore the teacher's multiscale representations. The method utilizes knowledge distillation, starting from abstract, high-level presentations to low-level features. The proposed approach achieves state-of-the-art performance in unsupervised anomaly detection, as demonstrated through extensive experimentation on AD and one-class novelty detection benchmarks.

**Anomaly Detection:**
- Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset.


**Motivation and Examples:**
- Anomaly detection is motivated by the need to detect unusual or suspicious activities, events, or patterns that may indicate potential fraud, errors, or security breaches.

- Examples of anomaly detection include detecting credit card fraud, network intrusion detection, identifying manufacturing defects, identifying medical anomalies in patient data, and detecting anomalies in sensor data.


**Main Categories of Anomaly Detection Methods:**
- Statistical Methods: These methods assume that normal data follows a specific statistical distribution, and anomalies are identified as data points that significantly deviate from this distribution. Suitable for low-dimensional data with known statistical properties.

- Machine Learning Methods: These methods use algorithms to learn patterns from normal data and identify anomalies as data points that do not conform to these learned patterns. Suitable for high-dimensional data with complex patterns.

- Deep Learning Methods: These methods leverage deep neural networks to learn complex representations of normal data and identify anomalies based on deviations from these learned representations. Suitable for high-dimensional data with complex patterns and non-linear relationships.

**Knowledge distillation improves unsupervised anomaly detection by:**

- **Diverse Anomalous Representations:** Unlike previous studies that use similar or identical architectures for the teacher and student models, knowledge distillation in this paper proposes a novel teacher-student (T-S) model with a teacher encoder and a student decoder. This approach enhances the diversity of anomalous representations .

- **Reverse Distillation Paradigm:** The paper introduces a "reverse distillation" paradigm, where the student network takes the teacher model's one-class embedding as input and aims to restore the teacher's multiscale representations. This knowledge distillation starts from abstract, high-level presentations to low-level features, improving the student's ability to discriminate anomalies .

- **One-Class Bottleneck Embedding (OCBE) Module:** The T-S model incorporates a trainable OCBE module, which effectively preserves essential information on normal patterns while disregarding anomaly perturbations. This compact embedding facilitates anomaly-free representation restoration at the student  .

- **Improved Performance:** Extensive experimentation on anomaly detection and one-class novelty detection benchmarks demonstrates that the proposed approach surpasses the state-of-the-art performance, highlighting the effectiveness and generalizability of the reverse distillation paradigm and OCBE module  .

---

    - What is Anomaly Detection
    - Motivation and Examples
    - Main categories of anomaly detection methods, including the basic idea of each type of methods and suitable scenarios to use (data types, dimensionality etc.). 


D. Gudovskiy, S. Ishizaka and K. Kozuka, "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows," 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Waikoloa, HI, USA, 2022, pp. 1819-1828, doi: 10.1109/WACV51458.2022.00188.


what are the Main categories of anomaly detection methods used in this paper? including the basic idea of each type of methods and suitable scenarios to use (data types, dimensionality etc.).

**Main categories of anomaly detection methods used in this paper:**

- **Unsupervised anomaly detection with localization**: The paper focuses on unsupervised anomaly detection methods that also provide localization of anomalies within the data. These methods are suitable when labeling is infeasible and when anomaly examples are completely missing in the training data .

- **Conditional normalizing flow framework**: The proposed CFLOW-AD model is based on a conditional normalizing flow framework. This framework allows for efficient anomaly detection with localization by modeling the complex distribution of encoded features and estimating the likelihood of these features .

- **Discriminatively pretrained encoder**: CFLOW-AD consists of a discriminatively pretrained encoder, which encodes the input data into a latent space representation. This encoder is trained to discriminate between normal and anomalous samples, enabling effective anomaly detection .

- **Multi-scale generative decoders**: CFLOW-AD also includes multi-scale generative decoders. These decoders estimate the likelihood of the encoded features and generate samples that closely match the original data distribution. The use of multiple scales allows for capturing both local and global characteristics of the encoded features, enhancing the model's anomaly detection and localization capabilities .

- **Suitable scenarios**: The CFLOW-AD model is designed for real-time anomaly detection with localization. It is particularly useful when labeling is not feasible, and when anomaly examples are missing in the training data. The model is computationally and memory-efficient, making it suitable for real-time processing. It has been evaluated on the MVTec dataset and outperforms previous methods in terms of detection and localization performance .

**Literature survey of this paper:**

- CFLOW-AD is a real-time unsupervised anomaly detection model with localization that achieves high accuracy metrics while being computationally and memory-efficient. It is based on a conditional normalizing flow framework adopted for anomaly detection with localization. CFLOW-AD consists of a discriminatively pretrained encoder followed by multi-scale generative decoders that estimate the likelihood of the encoded features. It outperforms previous methods on the MVTec dataset in both detection and localization tasks, with improvements of 0.36% AUROC in detection and 1.12% AUROC and 2.5% AUPRO in localization. The model is 10x faster and smaller than prior state-of-the-art models with the same input setting. The code for CFLOW-AD is open-source and the experiments are fully reproducible.

**Anomaly Detection:**
- Anomaly detection refers to the task of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset. It is used to detect unusual or rare events, outliers, or anomalies that may indicate potential problems or anomalies in the data.
 

**Motivation and Examples:**
- Anomaly detection has many practical applications, especially in scenarios where labeling is infeasible or when anomaly examples are completely missing in the training data. It can be used in various domains such as fraud detection, network intrusion detection, manufacturing quality control, and medical diagnosis.
 

**Main Categories of Anomaly Detection Methods:**
- Statistical Methods: These methods assume that anomalies are generated by a different statistical process than normal data. They use statistical techniques such as Gaussian distribution, clustering, or density estimation to identify anomalies.


- Machine Learning Methods: These methods use machine learning algorithms to learn patterns from normal data and then identify instances that deviate significantly from these learned patterns. They can include techniques such as clustering, classification, or regression.


- Information-Theoretic Methods: These methods use information theory to measure the deviation of data points from the expected behavior. They focus on quantifying the amount of information required to describe a data point and identify anomalies based on this measure.


- Proximity-Based Methods: These methods measure the similarity or dissimilarity between data points and identify anomalies based on their distance or proximity to other data points. They can include techniques such as nearest neighbor or clustering algorithms.


- Domain-Specific Methods: These methods are tailored to specific domains or data types. They leverage domain knowledge and specific characteristics of the data to detect anomalies effectively. For example, in image data, methods like CFLOW-AD use generative models to estimate the likelihood of encoded features for anomaly detection with localization.

**How does CFLOW-AD achieve real-time processing for anomaly detection?**

- CFLOW-AD achieves real-time processing for anomaly detection by proposing a computationally and memory-efficient model based on a conditional normalizing flow framework. It consists of a discriminatively pretrained encoder followed by multi-scale generative decoders that estimate the likelihood of the encoded features .
- The model is designed to be faster and smaller by a factor of 10x compared to prior state-of-the-art models with the same input setting .
- By adopting a conditional normalizing flow framework, CFLOW-AD is able to efficiently process data in real-time, making it suitable for applications where real-time processing is required .
- The model's efficiency allows it to achieve high accuracy metrics while maintaining real-time processing capabilities, making it a practical solution for unsupervised anomaly detection with localization .
- The experiments conducted on the MVTec dataset demonstrate that CFLOW-AD outperforms previous methods in both detection and localization tasks, further highlighting its effectiveness in real-time anomaly detection .

**Advantage of CFLOW-AD over prior state-of-the-art models:**

- CFLOW-AD offers a significant advantage over prior state-of-the-art models in terms of computational and memory efficiency, making it suitable for real-time processing  .
- It is faster and smaller by a factor of 10x compared to previous models with the same input setting, allowing for efficient processing of data in real-time .
- The model achieves high accuracy metrics while maintaining real-time processing capabilities, making it a practical solution for unsupervised anomaly detection with localization .
- CFLOW-AD is based on a conditional normalizing flow framework, which enables it to efficiently process data and estimate the likelihood of encoded features for anomaly detection with localization .
- Experimental results on the MVTec dataset demonstrate that CFLOW-AD outperforms previous methods in both detection and localization tasks, further highlighting its effectiveness in real-time anomaly detection .

**How does CFLOW-AD estimate likelihood of encoded features?**

- CFLOW-AD estimates the likelihood of encoded features through its multi-scale generative decoders, which are part of the model's architecture .
- The generative decoders explicitly estimate the likelihood of the encoded features, allowing CFLOW-AD to assess the anomaly score for each input sample .
- By using a conditional normalizing flow framework, CFLOW-AD is able to model the complex distribution of the encoded features and generate samples that closely match the original data distribution .
- The likelihood estimation is performed by the generative decoders at multiple scales, enabling CFLOW-AD to capture both local and global characteristics of the encoded features .
- This approach enhances the model's ability to accurately detect anomalies and localize them within the input data .
- The experiments conducted on the MVTec dataset demonstrate the effectiveness of CFLOW-AD in estimating the likelihood of encoded features, resulting in improved performance in both detection and localization tasks .

1. **Literature Review (25%)**
    - Overview of Clustering-Based Anomaly Detection Methods
    - Review of Representative Methods/Papers
    - Comparison of Methods (Pros, Cons, Similarities, Differences)

2. **Algorithm Description (20%)**
    - Definition and Rationale Behind Anomaly Score
    - Pseudo Code and Textual Explanation


Anomaly Detection Schemes 
General steps
Build a profile of the “normal” behavior
Profile can be patterns or summary statistics for the overall population
Use the “normal” profile to detect anomalies
Anomalies are observations whose characteristics differ significantly from the normal profile

### Anomaly Score Definition

#### Definition
The anomaly score `A(x)` for a given data point `x` is defined as the weighted sum of its Euclidean distance to its nearest cluster center and its distance rank within its cluster. Mathematically,

\[
A(x) = w_1 \times D(x, C_i) + w_2 \times R(x, C_i)
\]

- \(D(x, C_i)\) is the Euclidean distance between point \(x\) and its nearest cluster center \(C_i\).
- \(R(x, C_i)\) is the rank of \(x\) when all points in cluster \(C_i\) are sorted by their distance to \(C_i\).
- \(w_1\) and \(w_2\) are weights such that \(w_1 + w_2 = 1\).

#### Rationale

1. **Euclidean Distance (\(D\))**: Points far from their cluster centers are more likely to be anomalies. 
2. **Distance Rank (\(R\))**: It adds context by considering how a point's distance compares with other points in the same cluster.

#### Example

Suppose we have a cluster with points at distances `[1, 2, 2.5, 3, 10]` from the cluster center. If \(w_1 = 0.6\) and \(w_2 = 0.4\),

- A(x) for \(x\) at distance 10 would be \(0.6 \times 10 + 0.4 \times 5 = 8\).

### Algorithm Design

#### Pseudo-Code
```plaintext
1. Initialize K, w1, w2, and Anomaly_Threshold
2. Standardize the dataset
3. Train K-Means model on standardized dataset to get cluster centers C_1, C_2, ..., C_K
4. Initialize Anomaly_Scores as empty list
5. For each data point x in dataset:
    a. Find nearest cluster center C_i
    b. Calculate D(x, C_i) 
    c. Calculate R(x, C_i) within its cluster
    d. Calculate Anomaly_Score = w1 * D(x, C_i) + w2 * R(x, C_i)
    e. Append Anomaly_Score to Anomaly_Scores
6. For each Anomaly_Score in Anomaly_Scores:
    a. If Anomaly_Score > Anomaly_Threshold:
        Label corresponding data point as anomaly
```

#### Description of Steps

1. **Initialize Parameters**: Set the number of clusters \(K\), weights \(w_1\) and \(w_2\), and a threshold for flagging anomalies.
   
2. **Standardize Data**: Center and scale the data to have zero mean and unit variance. This is crucial for distance-based algorithms like K-means.

3. **Train K-Means**: Cluster the standardized dataset using K-means to obtain cluster centers.

4. **Initialize Anomaly Scores**: Create an empty list to store the anomaly scores.

5. **Calculate Anomaly Scores**: 
    - **Find Nearest Cluster**: For each point, find the nearest cluster center.
    - **Distance and Rank**: Calculate the Euclidean distance and distance rank for each point within its cluster.
    - **Compute Score**: Compute the anomaly score as the weighted sum of these distances and ranks.

6. **Label Anomalies**: Loop through the computed anomaly scores. If a score crosses the predefined threshold, label the corresponding data point as an anomaly.

#### Example

Suppose \(K = 3, w_1 = 0.6, w_2 = 0.4, \text{and Anomaly\_Threshold} = 7\). After running K-means, let's consider a data point \(x\) belonging to cluster \(C_1\). Assume \(D(x, C_1) = 5\) and \(R(x, C_1) = 3\).

- \( \text{Anomaly\_Score} = 0.6 \times 5 + 0.4 \times 3 = 4.2\)
- Since \(4.2 < 7\), \(x\) is not labeled as an anomaly.

This algorithm efficiently uses both distance and rank metrics, weighted appropriately, to identify anomalies. The use of a threshold allows for fine-tuning of the sensitivity of the anomaly detection process.

---


### Parametric vs Non-Parametric Methods

The K-means based approach is a non-parametric method because it does not make any assumptions about the underlying distribution of the data. It solely relies on the structure learned from the data.

### Parametric Anomaly Detection using Gaussian Mixture Model (GMM)

GMM is a probabilistic model that assumes that the data is generated from a mixture of several Gaussian distributions. The parameters of these distributions can be estimated from the data.

#### Anomaly Score Definition

The anomaly score `A(x)` for a data point `x` is defined as the negative log-likelihood of that point given the learned Gaussian Mixture Model:

\[
A(x) = -\log \left( \sum_{i=1}^{K} w_i \times \mathcal{N}(x; \mu_i, \Sigma_i) \right)
\]

- \( w_i \) is the weight of the \(i\)-th Gaussian component.
- \( \mathcal{N}(x; \mu_i, \Sigma_i) \) is the Gaussian distribution with mean \( \mu_i \) and covariance matrix \( \Sigma_i \).

#### Rationale

The lower the likelihood of observing a given data point under the fitted model, the higher its anomaly score. Thus, this score captures the anomalousness of data objects effectively.

#### Algorithm Design

##### Pseudo-Code

```plaintext
1. Initialize K, Anomaly_Threshold
2. Standardize the dataset
3. Fit a GMM model to the dataset, learn {w_i, mu_i, Sigma_i} for i = 1,...,K
4. Initialize Anomaly_Scores as empty list
5. For each data point x in dataset:
    a. Calculate A(x) using -log likelihood
    b. Append A(x) to Anomaly_Scores
6. For each Anomaly_Score in Anomaly_Scores:
    a. If Anomaly_Score > Anomaly_Threshold:
        Label corresponding data point as anomaly
```

##### Description of Steps

1. **Initialize Parameters**: Set \(K\) and the anomaly threshold.
2. **Standardize Data**: Standardization is essential for any distance-based model.
3. **Fit GMM**: Fit a Gaussian Mixture Model to the data.
4. **Initialize Anomaly Scores**: An empty list to store the calculated anomaly scores.
5. **Calculate Anomaly Scores**: Use the negative log-likelihood formula to calculate the anomaly score for each data point.
6. **Label Anomalies**: Any data point with an anomaly score greater than the threshold is labeled as an anomaly.

#### Example

Assume \(K = 3\) and \( \text{Anomaly\_Threshold} = 20\). After fitting the GMM, let's say we want to calculate the anomaly score for a data point \(x\).

- Assume \( w_1 = 0.5, \mu_1 = 0, \Sigma_1 = 1 \)
- Assume \( w_2 = 0.3, \mu_2 = 3, \Sigma_2 = 1 \)
- Assume \( w_3 = 0.2, \mu_3 = -3, \Sigma_3 = 1 \)

The anomaly score \( A(x) \) would be calculated using the negative log-likelihood formula.

- If \( A(x) > 20 \), \(x\) is labeled as an anomaly.

The parametric GMM-based method allows us to explicitly model the underlying distributions, making the algorithm's assumptions clear and enabling a probabilistic interpretation of anomalies.

---

### K-means Based Anomaly Detection Algorithm

#### Definition of the Anomaly Score

The anomaly score `A(x)` for a data point `x` is defined as the normalized distance from the point to its nearest cluster center multiplied by the cluster's standard deviation. The formula is:

\[
A(x) = \frac{D(x, C_i)}{\max(D)} \times \sigma(C_i)
\]

- \(D(x, C_i)\) is the Euclidean distance between point \(x\) and its nearest cluster center \(C_i\).
- \(\max(D)\) is the maximum distance any point has to its nearest cluster center across all clusters.
- \(\sigma(C_i)\) is the standard deviation of distances of all points in cluster \(C_i\) to \(C_i\).

#### Rationale

- **Normalized Distance**: Normalizing the distance by the maximum distance across all clusters puts all points on a comparable scale.
  
- **Cluster Variance (\(\sigma\))**: Multiplying by the standard deviation of the distances in the cluster accounts for the cluster's density. Points in sparse clusters are more likely to be anomalous.

#### Example

Suppose a point \(x\) has a distance of 10 units from its nearest cluster center \(C_1\), the maximum distance across all clusters is 20, and the standard deviation of distances in \(C_1\) is 2. Then, \( A(x) = \frac{10}{20} \times 2 = 1 \).

#### Pseudo-Code

```plaintext
1. Initialize K, Anomaly_Threshold
2. Standardize the dataset
3. Train K-means model to get cluster centers C_1, ..., C_K
4. Calculate max(D) across all clusters
5. Initialize Anomaly_Scores = []
6. For each point x in dataset:
    a. Find nearest cluster center C_i
    b. Calculate D(x, C_i)
    c. Calculate sigma(C_i)
    d. A(x) = (D(x, C_i) / max(D)) * sigma(C_i)
    e. Append A(x) to Anomaly_Scores
7. For each A(x) in Anomaly_Scores:
    a. If A(x) > Anomaly_Threshold:
        Label x as anomaly
```

#### Description of Steps

1. **Initialize Parameters**: Choose \(K\) and set the anomaly threshold.
2. **Standardize Data**: Center and scale the data.
3. **Train K-means**: Obtain cluster centers.
4. **Calculate Max Distance**: Find the maximum distance any point has to its nearest cluster center.
5. **Initialize Anomaly Scores**: Create an empty list.
6. **Calculate Anomaly Scores**: For each data point, calculate \(A(x)\) as defined.
7. **Label Anomalies**: Label points as anomalies if their \(A(x)\) exceeds the threshold.




8. **Application to Dataset (30%)**
    - Algorithm Execution Steps
    - Detected Anomalies and Discussion
    - Data Pre-processing Steps (if needed)

### 5-Day Plan

**Day 1: Research**
- Search and review literature focusing on clustering-based anomaly detection methods. Aim to finalize 8-10 related papers for an in-depth review.
  
**Day 2: Introduction & Literature Review Writing**
- Write the Introduction.
- Write the Literature Review based on the selected papers.

**Day 3: Algorithm Design**
- Sketch out the anomaly score, considering factors from k-means that will help in detecting anomalies.
- Formulate the algorithm in pseudo code and provide a text-based explanation.

**Day 4: Application and Experimentation**
- Use SAS Enterprise Miner for k-means clustering on `a2-housing.csv`.
- Apply other steps of the algorithm using tools at your disposal, identify anomalies, and evaluate results.

**Day 5: Finalization and Review**
- Write the section on applying the algorithm to the dataset.
- Review the entire paper, ensuring that it meets all the specified requirements and is error-free. 

